{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGaFoeq0gMsk"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aSZPFOSegMsk",
    "outputId": "527f3f33-e45b-4a6c-f11d-3bb8d75dacdf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "#import tensorflow_probability as tfp\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BG9UkSzAzTt-"
   },
   "outputs": [],
   "source": [
    "\n",
    "def parse_images(example):\n",
    "    feature_description = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"path\": tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "\n",
    "    example = tf.io.parse_single_example(example, feature_description)\n",
    "    image = tf.io.decode_png(example[\"image\"], channels=1)\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DPtzoPbF_BMf"
   },
   "outputs": [],
   "source": [
    "vehicles_dset = tf.data.TFRecordDataset(\"vehicles.tfrecord\")\n",
    "img_dset = vehicles_dset.map(parse_images)\n",
    "img_dset = img_dset.batch(64)\n",
    "img_dset = img_dset.shuffle(1000)\n",
    "img_dset = img_dset.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "id": "9N3PFleZuHrq",
    "outputId": "924f1205-5531-48b3-d851-d234597576c6"
   },
   "outputs": [],
   "source": [
    "iterator = iter(img_dset)\n",
    "cur_image = next(iterator)\n",
    "#print(image.numpy().shape)\n",
    "plt.imshow(cur_image[0].numpy(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_fWm6mGgMsl"
   },
   "outputs": [],
   "source": [
    "\n",
    "class VectorQuantizer(layers.Layer):\n",
    "    def __init__(self, num_embeddings, embedding_dim, beta=0.25, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "\n",
    "        # The `beta` parameter is best kept between [0.25, 2] as per the paper.\n",
    "        self.beta = beta\n",
    "\n",
    "        # Initialize the embeddings which we will quantize.\n",
    "        w_init = tf.random_uniform_initializer()\n",
    "        self.embeddings = tf.Variable(\n",
    "            initial_value=w_init(\n",
    "                shape=(self.embedding_dim, self.num_embeddings), dtype=\"float32\"\n",
    "            ),\n",
    "            trainable=True,\n",
    "            name=\"embeddings_vqvae\",\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        # Calculate the input shape of the inputs and\n",
    "        # then flatten the inputs keeping `embedding_dim` intact.\n",
    "        input_shape = tf.shape(x)\n",
    "        flattened = tf.reshape(x, [-1, self.embedding_dim])\n",
    "\n",
    "        # Quantization.\n",
    "        encoding_indices = self.get_code_indices(flattened)\n",
    "        encodings = tf.one_hot(encoding_indices, self.num_embeddings)\n",
    "        quantized = tf.matmul(encodings, self.embeddings, transpose_b=True)\n",
    "\n",
    "        # Reshape the quantized values back to the original input shape\n",
    "        quantized = tf.reshape(quantized, input_shape)\n",
    "\n",
    "        # Calculate vector quantization loss and add that to the layer. You can learn more\n",
    "        # the original paper to get a handle on the formulation of the loss function.\n",
    "        commitment_loss = tf.reduce_mean((tf.stop_gradient(quantized) - x) ** 2)\n",
    "        codebook_loss = tf.reduce_mean((quantized - tf.stop_gradient(x)) ** 2)\n",
    "        self.add_loss(self.beta * commitment_loss + codebook_loss)\n",
    "\n",
    "        # Straight-through estimator.\n",
    "        quantized = x + tf.stop_gradient(quantized - x)\n",
    "        return quantized\n",
    "\n",
    "    def get_code_indices(self, flattened_inputs):\n",
    "        # Calculate L2-normalized distance between the inputs and the codes.\n",
    "        similarity = tf.matmul(flattened_inputs, self.embeddings)\n",
    "        distances = (\n",
    "            tf.reduce_sum(flattened_inputs ** 2, axis=1, keepdims=True)\n",
    "            + tf.reduce_sum(self.embeddings ** 2, axis=0)\n",
    "            - 2 * similarity\n",
    "        )\n",
    "\n",
    "        # Derive the indices for minimum distances.\n",
    "        encoding_indices = tf.argmin(distances, axis=1)\n",
    "        return encoding_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a1VHo5nQgMsm"
   },
   "outputs": [],
   "source": [
    "def get_encoder(latent_dim=16):\n",
    "    encoder_inputs = keras.Input(shape=(120, 240, 1))\n",
    "    x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    encoder_outputs = layers.Conv2D(latent_dim, 1, padding=\"same\")(x)\n",
    "    return keras.Model(encoder_inputs, encoder_outputs, name=\"encoder\")\n",
    "\n",
    "\n",
    "def get_decoder(latent_dim=16):\n",
    "    latent_inputs = keras.Input(shape=(15, 30, latent_dim))\n",
    "    x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(latent_inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "    return keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hB_yb6OMgMsm"
   },
   "source": [
    "## Standalone VQ-VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YrPl_DMAgMsm",
    "outputId": "510ea32c-e0de-493e-c776-c114dfafceb1"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_vqvae(latent_dim=16, num_embeddings=512):\n",
    "    vq_layer = VectorQuantizer(num_embeddings, latent_dim, name=\"vector_quantizer\")\n",
    "    encoder = get_encoder(latent_dim)\n",
    "    decoder = get_decoder(latent_dim)\n",
    "    inputs = keras.Input(shape=(120, 240, 1))\n",
    "    encoder_outputs = encoder(inputs)\n",
    "    quantized_latents = vq_layer(encoder_outputs)\n",
    "    reconstructions = decoder(quantized_latents)\n",
    "    return keras.Model(inputs, reconstructions, name=\"vq_vae\")\n",
    "\n",
    "\n",
    "get_vqvae().summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369
    },
    "id": "p47ShipSgwPG",
    "outputId": "286d03f2-ba57-4d55-f463-acaec587f00d"
   },
   "outputs": [],
   "source": [
    "keras.utils.plot_model(get_vqvae())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YH70x7TUgMsn"
   },
   "source": [
    "## Wrapping up the training loop inside `VQVAETrainer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lThvP6t8gMsn"
   },
   "outputs": [],
   "source": [
    "\n",
    "class VQVAETrainer(keras.models.Model):\n",
    "    def __init__(self, train_variance, latent_dim=16, num_embeddings=64, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.train_variance = train_variance\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "\n",
    "        self.vqvae = get_vqvae(self.latent_dim, self.num_embeddings)\n",
    "\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.vq_loss_tracker = keras.metrics.Mean(name=\"vq_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.vq_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, x):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Outputs from the VQ-VAE.\n",
    "            reconstructions = self.vqvae(x)\n",
    "\n",
    "            # Calculate the losses.\n",
    "            reconstruction_loss = (\n",
    "                tf.reduce_mean((x - reconstructions) ** 2) / self.train_variance\n",
    "            )\n",
    "            total_loss = reconstruction_loss + sum(self.vqvae.losses)\n",
    "\n",
    "        # Backpropagation.\n",
    "        grads = tape.gradient(total_loss, self.vqvae.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.vqvae.trainable_variables))\n",
    "\n",
    "        # Loss tracking.\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.vq_loss_tracker.update_state(sum(self.vqvae.losses))\n",
    "\n",
    "        # Log results.\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"vqvae_loss\": self.vq_loss_tracker.result(),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rNoIpMs-gMsn",
    "outputId": "26b36005-1238-4c9d-8bfa-5cc911b059f2"
   },
   "outputs": [],
   "source": [
    "# Step 3: Convert the dataset to a numpy array for computing data variance\n",
    "scaled_images = np.concatenate(list(img_dset.as_numpy_iterator()), axis=0)\n",
    "\n",
    "# Step 4: Compute the data variance\n",
    "data_variance = np.var(scaled_images)\n",
    "print(data_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0sGpGe6xgMsn",
    "outputId": "1ec26932-4bcd-45e1-dce8-2b21186ec860"
   },
   "outputs": [],
   "source": [
    "vqvae_trainer = VQVAETrainer(data_variance, latent_dim=16, num_embeddings=128)\n",
    "vqvae_trainer.compile(optimizer=keras.optimizers.Adam())\n",
    "vqvae_trainer.fit(img_dset,epochs=40, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSBWE48rgMsn"
   },
   "source": [
    "## Reconstruction results on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "DaI4tzndPjOd",
    "outputId": "205d39d3-59f4-4471-a785-9f5b6d12b742"
   },
   "outputs": [],
   "source": [
    "# Your existing code\n",
    "iterator = iter(img_dset)\n",
    "next_sample = next(iterator)\n",
    "\n",
    "first_img = next_sample\n",
    "predict_first = vqvae_trainer.vqvae(first_img)\n",
    "\n",
    "# Plot both images side by side using subplots\n",
    "fig, axes = plt.subplots(5, 2, figsize=(10, 15))\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    axes[i, 0].imshow(first_img[i, :, :, 0], cmap='gray')\n",
    "    axes[i, 0].set_title('Original Image')\n",
    "\n",
    "    axes[i, 1].imshow(predict_first[i, :, :, 0], cmap='gray')\n",
    "    axes[i, 1].set_title('Predicted Image')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rKIacDH9gMsn",
    "outputId": "e17fc5d8-2ed9-4adf-8b88-6d45028b4993"
   },
   "outputs": [],
   "source": [
    "\n",
    "def show_subplot(original, reconstructed):\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(original.squeeze() + 0.5 , cmap='gray')\n",
    "    plt.title(\"Original\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(reconstructed.squeeze() + 0.5, cmap='gray')\n",
    "    plt.title(\"Reconstructed\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "trained_vqvae_model = vqvae_trainer.vqvae\n",
    "idx = np.random.choice(len(scaled_images), 10)\n",
    "test_images = scaled_images[idx]\n",
    "reconstructions_test = trained_vqvae_model.predict(test_images)\n",
    "\n",
    "for test_image, reconstructed_image in zip(test_images, reconstructions_test):\n",
    "    show_subplot(test_image, reconstructed_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKLxgyUagMso"
   },
   "source": [
    "## Visualizing the discrete codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "aeiJyN1bgMso",
    "outputId": "09865deb-9432-4e1b-95c7-87f840fa281b"
   },
   "outputs": [],
   "source": [
    "encoder = vqvae_trainer.vqvae.get_layer(\"encoder\")\n",
    "quantizer = vqvae_trainer.vqvae.get_layer(\"vector_quantizer\")\n",
    "\n",
    "encoded_outputs = encoder.predict(test_images)\n",
    "flat_enc_outputs = encoded_outputs.reshape(-1, encoded_outputs.shape[-1])\n",
    "codebook_indices = quantizer.get_code_indices(flat_enc_outputs)\n",
    "codebook_indices = codebook_indices.numpy().reshape(encoded_outputs.shape[:-1])\n",
    "\n",
    "for i in range(len(test_images)):\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(test_images[i].squeeze() + 0.5)\n",
    "    plt.title(\"Original\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(codebook_indices[i])\n",
    "    plt.title(\"Code\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
